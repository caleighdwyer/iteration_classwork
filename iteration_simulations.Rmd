---
title: "iteration_simulations"
author: "Caleigh Dwyer"
date: "2023-11-02"
output: github_document
---

```{r}
library(tidyverse)
library(rvest)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```



```{r}
sim_mean_sd = function(n_obs, mu = 5, sigma = 1){
  x_vec = rnorm(n = n_obs, mean = mu, sd = sigma)
  
  tibble(
    mean = mean(x_vec),
    sd = sd(x_vec)
  )
}
```


```{r}
sim_mean_sd(n_obs = 30)

#what is the variability we should expect when sampling with this amount? if you could draw from a population repeatedly, your samples would resemble a known distribution

```

Let's iterate to see how this works under repeated sampling

```{r}
output = vector("list", length = 100)

for (i in 1:100){
  output[[i]] = sim_mean_sd(n_obs = 30)
  
}

sim_results = 
  bind_rows(output)

##this provides a two column tibble when bind_rows

sim_results |> 
  ggplot(aes(x = mean)) + geom_density()

##given that we defined the true population mean (mu) as 5, makes sense that the mean of our sampled distribution of means = 5


sim_results |> 
  summarize(
    mu_hat = mean(mean),
    sd_hat = sd(mean)
  )

##makes sense that sd of sampled means is about 1/sqrt(30)
```

use a map function instead of for loop

```{r}

sim_result_df = 
  expand_grid(
    sample_size = c(30, 60, 120, 240),
    iter = 1:1000
  ) |> 
  mutate(estimate_df = map(sample_size, sim_mean_sd)) |> 
  unnest(estimate_df)

sim_result_df |> 
  mutate(
    sample_size = str_c("n =", sample_size),
sample_size = fct_inorder(sample_size))|> 
  ggplot(aes(x = sample_size, y = mean)) +
  geom_boxplot()

##result shows that as sample size increases, variability decreases
```



```{r}
sim_mean_sd_2 = function(n_obs, true_p = .9){
  x_vec = rbinom(n = n_obs, size =1, prob = true_p)
  
  tibble(
    mean = mean(x_vec),
    sd = sd(x_vec)
  )
}

## you could replace this with the sim_mean_sd chunk above and it would run all the same code for a binomial distribution (which shows probability from 0 to 1)
```

simulation of linear regression (simple linear regression, SLR)

goal is to write a function that simulates data and then fits a regression; then repeat to look at the distibturion of estimated coefficients. 

```{r}
beta_0 = 2
beta_1 = 3

sim_data =
  tibble(
    x = rnorm(n = 30, mean = 1, sd = 1),
    y = beta_0 + beta_1 * x + rnorm(30, mean = 0, sd = 1)
  )

ls_fit = lm(y ~ x, data = sim_data)
ls_fit

sim_data |> 
  ggplot(aes(x = x, y =y))+
  geom_point()
```


let's wrap this in a function

```{r}
sim_slr = function(n_obs, beta_0 = 2, beta_1 = 3){
sim_data =
  tibble(
    x = rnorm(n = n_obs, mean = 1, sd = 1),
    y = beta_0 + beta_1 * x + rnorm(30, mean = 0, sd = 1)
  )

ls_fit = lm(y ~ x, data = sim_data)

tibble(
  beta0_hat = coef(ls_fit)[1],
  beta1_hat = coef(ls_fit)[2]
)
}

sim_slr(n_obs = 30)

```


run this a whole bunch of times

```{r}
sim_results_df = expand_grid(
  sample_size = 30,
  iter = 1:1000
) |> 
  mutate(estimate_df = map(sample_size, sim_slr)) |> 
  unnest(estimate_df)
```

let's look at results

```{r}
sim_results_df |> 
  summarize(
    mean_b0_hat = mean(beta0_hat),
    mean_b1_hat = mean(beta1_hat)
  )

sim_result_df |> 
  ggplot(aes(x= beta0_hat))+
  geom_histogram()

##didn't work for me

sim_results_df |> 
  ggplot(aes(x= beta0_hat, y = beta1_hat))+
  geom_point()

##regression coefficients are correlated with each other (demonstrated by graph above)
```

try to always use tidyverse over base r, even in very similar functions (like read_csv vs. read.csv (base))
